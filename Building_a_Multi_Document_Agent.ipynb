{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anuragkumar130896/Agentic-rag-router-agents-project/blob/main/Building_a_Multi_Document_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b523e0a",
      "metadata": {
        "id": "0b523e0a"
      },
      "source": [
        "# Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a323703",
      "metadata": {
        "id": "1a323703"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f"
      },
      "outputs": [],
      "source": [
        "from helper import get_openai_api_key\n",
        "OPENAI_API_KEY = get_openai_api_key()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3221a474-5817-4db2-af46-e029042a75a5",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "3221a474-5817-4db2-af46-e029042a75a5"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20adaa26",
      "metadata": {
        "id": "20adaa26"
      },
      "source": [
        "## 1. Setup an agent over 3 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
      "metadata": {
        "height": 200,
        "tags": [],
        "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
        "outputId": "93981f11-754b-444c-eb5c-78dad744ad33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07"
      },
      "outputs": [],
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff58c52",
      "metadata": {
        "height": 64,
        "id": "bff58c52"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2c6a9f",
      "metadata": {
        "height": 30,
        "id": "2f2c6a9f",
        "outputId": "d189ceef-eaf8-4f31-c3f0-526ab0939b03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(initial_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a124a438-5609-402e-8642-69d1088cb9ad",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "a124a438-5609-402e-8642-69d1088cb9ad"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
        "outputId": "ac18ef64-d609-43fa-fa4d-829b184710b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "PG19 test split\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results even on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
            "\n",
            "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results even on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace340b1-761f-4058-be41-68cf131541e4",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "ace340b1-761f-4058-be41-68cf131541e4",
        "outputId": "28cc7df3-2323-43f3-ca85-a6e074341bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
            "=== Function Output ===\n",
            "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It trains a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on its own generations using special tokens called reflection tokens. By incorporating retrieval and self-reflection, Self-RAG aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
            "=== Function Output ===\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs compared to full fine-tuning. It introduces shifted sparse attention (S2-Attn) combined with LoRA to extend context sizes, enabling significant computation savings while maintaining performance. LongLoRA allows for efficiently extending the context window of LLMs, demonstrating strong empirical results on various tasks and models. It bridges the performance gap between LoRA and full fine-tuning by enabling the training of normalization and embedding layers, crucial for adapting LLMs to longer contexts. Additionally, LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
            "=== LLM Response ===\n",
            "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It trains a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on its own generations using special tokens called reflection tokens. By incorporating retrieval and self-reflection, Self-RAG aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
            "\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs compared to full fine-tuning. It introduces shifted sparse attention (S2-Attn) combined with LoRA to extend context sizes, enabling significant computation savings while maintaining performance. LongLoRA allows for efficiently extending the context window of LLMs, demonstrating strong empirical results on various tasks and models. It bridges the performance gap between LoRA and full fine-tuning by enabling the training of normalization and embedding layers, crucial for adapting LLMs to longer contexts. Additionally, LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
            "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It trains a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on its own generations using special tokens called reflection tokens. By incorporating retrieval and self-reflection, Self-RAG aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
            "\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs compared to full fine-tuning. It introduces shifted sparse attention (S2-Attn) combined with LoRA to extend context sizes, enabling significant computation savings while maintaining performance. LongLoRA allows for efficiently extending the context window of LLMs, demonstrating strong empirical results on various tasks and models. It bridges the performance gap between LoRA and full fine-tuning by enabling the training of normalization and embedding layers, crucial for adapting LLMs to longer contexts. Additionally, LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eede70c",
      "metadata": {
        "id": "7eede70c"
      },
      "source": [
        "## 2. Setup an agent over 11 papers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18771e69",
      "metadata": {
        "id": "18771e69"
      },
      "source": [
        "### Download 11 ICLR papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
      "metadata": {
        "height": 472,
        "tags": [],
        "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    \"vr_mcl.pdf\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77426cb",
      "metadata": {
        "tags": [],
        "id": "b77426cb"
      },
      "source": [
        "To download these papers, below is the needed code:\n",
        "\n",
        "\n",
        "    #for url, paper in zip(urls, papers):\n",
        "         #!wget \"{url}\" -O \"{paper}\"\n",
        "    \n",
        "    \n",
        "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
        "outputId": "08a6a5c9-99ba-4714-9b13-25765cc2d175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n",
            "Getting tools for paper: swebench.pdf\n",
            "Getting tools for paper: selfrag.pdf\n",
            "Getting tools for paper: zipformer.pdf\n",
            "Getting tools for paper: values.pdf\n",
            "Getting tools for paper: finetune_fair_diffusion.pdf\n",
            "Getting tools for paper: knowledge_card.pdf\n",
            "Getting tools for paper: metra.pdf\n",
            "Getting tools for paper: vr_mcl.pdf\n"
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e35d52c",
      "metadata": {
        "id": "4e35d52c"
      },
      "source": [
        "### Extend the Agent with Tool Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "20154923-873e-4941-9a3a-4926ab5f9b8c"
      },
      "outputs": [],
      "source": [
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "671582f9-70d7-4a8f-b813-58b2a068ca72"
      },
      "outputs": [],
      "source": [
        "# define an \"object\" index and retriever over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c3929882-e9dc-46ca-b495-53e3ed60340e"
      },
      "outputs": [],
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03"
      },
      "outputs": [],
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
        "outputId": "37cd7c1d-2fac-4fd1-c2c8-30dfc7475663"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tools[2].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
      "metadata": {
        "height": 251,
        "tags": [],
        "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries over a set of given papers.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
        "outputId": "44c39a97-0714-494b-80c4-233589d4c5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and the SoftwareDev dataset, which consists of diverse software development tasks like creating games, processing data, managing CRUD operations, transcribing music, generating press releases, and developing applications like weather dashboards.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset is used to evaluate models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama, assessing their performance in resolving issues using BM25 retrieval and the \"oracle\" retrieval setting. The dataset is constructed from public repositories with licenses permitting software usage, and it aims to cover more programming languages and domains in the future. The dataset does not involve human subject participation, crowdsourcing, or biased heuristics for repository selection. Task instances in the dataset are validated through execution-based validation to ensure usability and are continuously updated to maintain temporal robustness.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and the SoftwareDev dataset, which consists of diverse software development tasks like creating games, processing data, managing CRUD operations, transcribing music, generating press releases, and developing applications like weather dashboards.\n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset is used to evaluate models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama, assessing their performance in resolving issues using BM25 retrieval and the \"oracle\" retrieval setting. The dataset is constructed from public repositories with licenses permitting software usage, and it aims to cover more programming languages and domains in the future. The dataset does not involve human subject participation, crowdsourcing, or biased heuristics for repository selection. Task instances in the dataset are validated through execution-based validation to ensure usability and are continuously updated to maintain temporal robustness.\n",
            "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and the SoftwareDev dataset, which consists of diverse software development tasks like creating games, processing data, managing CRUD operations, transcribing music, generating press releases, and developing applications like weather dashboards.\n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset is used to evaluate models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama, assessing their performance in resolving issues using BM25 retrieval and the \"oracle\" retrieval setting. The dataset is constructed from public repositories with licenses permitting software usage, and it aims to cover more programming languages and domains in the future. The dataset does not involve human subject participation, crowdsourcing, or biased heuristics for repository selection. Task instances in the dataset are validated through execution-based validation to ensure usability and are continuously updated to maintain temporal robustness.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used \"\n",
        "    \"in MetaGPT and compare it against SWE-Bench\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
        "outputId": "409523fb-4155-4bc0-c7fa-017147b5a95d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
            "=== Function Output ===\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning of models to longer context lengths without significantly increasing memory requirements or training time. This approach aims to bridge the performance gap between short and long context lengths, achieving comparable results to full fine-tuning with reduced computational overhead.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ\"}\n",
            "=== Function Output ===\n",
            "LoftQ is a quantization framework designed for Large Language Models (LLMs) that integrates quantization and Low-Rank Adaptation (LoRA) fine-tuning. It aims to bridge the performance gap between quantized and full-precision models by simultaneously quantizing an LLM and finding a suitable low-rank initialization for LoRA fine-tuning. This framework significantly enhances generalization in downstream tasks, especially in challenging low-bit precision scenarios, and has shown superior effectiveness compared to existing methods.\n",
            "=== LLM Response ===\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning of models to longer context lengths without significantly increasing memory requirements or training time. This approach aims to bridge the performance gap between short and long context lengths, achieving comparable results to full fine-tuning with reduced computational overhead.\n",
            "\n",
            "LoftQ, on the other hand, is a quantization framework designed for Large Language Models (LLMs) that integrates quantization and Low-Rank Adaptation (LoRA) fine-tuning. It aims to bridge the performance gap between quantized and full-precision models by simultaneously quantizing an LLM and finding a suitable low-rank initialization for LoRA fine-tuning. This framework significantly enhances generalization in downstream tasks, especially in challenging low-bit precision scenarios, and has shown superior effectiveness compared to existing methods.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "    \"Analyze the approach in each paper first. \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e933db65",
      "metadata": {
        "height": 30,
        "id": "e933db65"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}